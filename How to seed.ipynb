{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8410b2",
   "metadata": {},
   "source": [
    "# How can you seed with an LLM?\n",
    "\n",
    "With a temperature of 0, the response of an AI agent will be the same every time. But what if you want a reproducable output that isn't deterministic?\n",
    "\n",
    "Can a seed change that response meaningfully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814f7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from creds import openai_key\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58feedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"seed\"],\n",
    "    template=\"\"\"\n",
    "    Give me one interesting baby name. Generate the name randomly with a seed of {seed}. \n",
    "    \"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc3f8a",
   "metadata": {},
   "source": [
    "With the same seed, we see no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7017acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n"
     ]
    }
   ],
   "source": [
    "seeds = [1]*10\n",
    "\n",
    "for seed in seeds:\n",
    "    print(llm_chain.run(seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329196a",
   "metadata": {},
   "source": [
    "With numeric seeds, there is a tiny bit of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486ab1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avery\n",
      "Zephyr\n",
      "Zephyr\n",
      "Aria\n",
      "Zephyr\n",
      "Aria\n",
      "Aria\n",
      "Aria\n",
      "Aria\n",
      "Aria\n"
     ]
    }
   ],
   "source": [
    "seeds = range(10)\n",
    "\n",
    "for seed in seeds:\n",
    "    print(llm_chain.run(seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dba8c6",
   "metadata": {},
   "source": [
    "With larger more random numeric seeds, we don't see any benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a002ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zephyr\n",
      "Aria\n",
      "Aria\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n",
      "Zephyr\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate 10 large random integers\n",
    "seeds = [random.randint(1000000000, 9999999999) for _ in range(10)]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(llm_chain.run(seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304b32b",
   "metadata": {},
   "source": [
    "Using words instead for the seed means we need to rephrase the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c53e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"seed\"],\n",
    "    template=\"\"\"\n",
    "    Give me one interesting baby name. Be inspired by the word {seed}. \n",
    "    \"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f59d55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "process\n",
      "its\n",
      "east\n",
      "help\n",
      "plant\n",
      "occur\n",
      "develop\n",
      "different\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "\n",
    "# Create a Faker object\n",
    "faker = Faker()\n",
    "\n",
    "# Generate 10 random words\n",
    "seeds = [faker.word() for _ in range(10)]\n",
    "\n",
    "# Display the random words\n",
    "for word in seeds:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e819fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saylor\n",
      "Procesia\n",
      "Irisa - inspired by the word \"iris\", which is a beautiful flower with a unique shape and vibrant colors.\n",
      "Easterly\n",
      "Helperia\n",
      "Sagebrush\n",
      "Ocira (pronounced oh-SEER-ah)\n",
      "Devlyn - a unique and modern name inspired by the word \"develop\" that means \"one who is skilled in developing or growing something.\"\n",
      "Dyfferentia (pronounced \"dih-fuh-ren-shuh\")\n",
      "Badrinath - a unique and beautiful name inspired by the word \"bad\" which means \"good\" in Arabic. It is also the name of a famous Hindu pilgrimage site in India.\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    print(llm_chain.run(seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276efcf1",
   "metadata": {},
   "source": [
    "Those sure are some terrible names. A bit of prompt engineering can fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39cf79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"seed\"],\n",
    "    template=\"\"\"\n",
    "    Give me one interesting baby name. Be very loosely inspired by the word {seed}. Don't explain yourself, just return the name.\n",
    "    \"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "241aed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saylor\n",
      "Prosecco\n",
      "Istella\n",
      "Ezra\n",
      "Zelpha\n",
      "Sage\n",
      "Ocelyn\n",
      "Devlin\n",
      "Zephyr\n",
      "Baxter\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    print(llm_chain.run(seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268468d1",
   "metadata": {},
   "source": [
    "Kinda works. With some more prompt engineering, this word based seed approach could unlock reproducability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
